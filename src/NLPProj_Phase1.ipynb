{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPProj_Phase1",
      "provenance": [],
      "collapsed_sections": [
        "tI7wTx6Y86aA",
        "n_C_B6QhjNPt",
        "8dReN9IjZScj",
        "N6pkkl88-AjE",
        "aSZUnmekZkzr",
        "qJ9riIqArCA8"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R977d61QZ5Ns"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx87-M3UDuD3"
      },
      "source": [
        "! pip install hazm\n",
        "! pip install langdetect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rz49iAGFy7d"
      },
      "source": [
        "# Import needed libraries\n",
        "\n",
        "from google.colab import drive  # to mount Google Drive to Colab notebook\n",
        "import tweepy                   # Python wrapper around Twitter API\n",
        "import json\n",
        "import pandas as pd\n",
        "import csv\n",
        "from datetime import date\n",
        "from datetime import datetime\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.plotting import table\n",
        "\n",
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "from langdetect import detect\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ4bxxjf2_74"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7Dht9hN7zLt"
      },
      "source": [
        "# Twitter Data Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI7wTx6Y86aA"
      },
      "source": [
        "## Log into Twitter API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0sLihQvDfqf"
      },
      "source": [
        "# Load Twitter API secrets from an external file\n",
        "path = '/content/drive/MyDrive/Project/'\n",
        "secrets = json.loads(open(path + 'secrets.json').read())  \n",
        "\n",
        "consumer_key = secrets['consumer_key']\n",
        "consumer_secret = secrets['consumer_secret']\n",
        "access_token = secrets['access_token']\n",
        "access_token_secret = secrets['access_token_secret']\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "api = tweepy.API(auth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_C_B6QhjNPt"
      },
      "source": [
        "## Checking the connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0IawailjFqA"
      },
      "source": [
        "public_tweets = api.home_timeline()\n",
        "for tweet in public_tweets:\n",
        "    print(tweet.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dReN9IjZScj"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7UpX8xgW9n6"
      },
      "source": [
        "# Helper function to save data into a JSON file\n",
        "# file_name: the name of the data file you want to save on your Google Drive\n",
        "# file_content: the data you want to save\n",
        "\n",
        "def save_json(file_name, file_content):\n",
        "  with open(path + file_name, 'w', encoding='utf-8') as f:\n",
        "    json.dump(file_content, f, ensure_ascii=False, indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ocl22Jop1OmM"
      },
      "source": [
        "# Helper function to handle twitter API rate limit\n",
        "\n",
        "def limit_handled(cursor, list_name):\n",
        "    while True:\n",
        "        try:\n",
        "            yield cursor.next()\n",
        "        except tweepy.RateLimitError:\n",
        "            print(\"\\nCurrent number of data points in list = \" + str(len(list_name)))\n",
        "            print('Hit Twitter API rate limit.')\n",
        "            for i in range(3, 0, -1):\n",
        "              print(\"Wait for {} mins.\".format(i * 5))\n",
        "              time.sleep(5 * 60)\n",
        "        except tweepy.error.TweepError:\n",
        "            print('\\nCaught TweepError exception' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6pkkl88-AjE"
      },
      "source": [
        "## Get All Tweets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GgkDnT7cplW"
      },
      "source": [
        "def get_all_tweets(screen_name):\n",
        "\n",
        "  # path = '/content/drive/MyDrive/Project/data/raw'\n",
        "\n",
        "\n",
        "\t# initialize a list to hold all the tweepy Tweets\n",
        "\talltweets = []\n",
        "\t\n",
        "\t# make initial request for most recent tweets (200 is the maximum allowed count)\n",
        "\tnew_tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
        "\t\n",
        "\t# save most recent tweets\n",
        "\talltweets.extend(new_tweets)\n",
        "\t\n",
        "\t# save the id of the oldest tweet less one\n",
        "\toldest = alltweets[-1].id - 1\n",
        "\t\n",
        "\t# keep grabbing tweets until there are no tweets left to grab\n",
        "\twhile len(new_tweets) > 0:\n",
        "\t\t# print(\"getting tweets before %s\" % (oldest))\n",
        "\t\t\n",
        "\t\t# all subsiquent requests use the max_id param to prevent duplicates\n",
        "\t\tnew_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)\n",
        "\t\t\n",
        "\t\t# save most recent tweets\n",
        "\t\talltweets.extend(new_tweets)\n",
        "\t\t\n",
        "\t\t# update the id of the oldest tweet less one\n",
        "\t\toldest = alltweets[-1].id - 1\n",
        "\t\t\n",
        "\t# print(\"...%s done\" % (screen_name)\n",
        "\t\n",
        "\t# transform the tweepy tweets into a 2D array that will populate the csv\t\n",
        "\touttweets = [[tweet.id_str, tweet.created_at, tweet.text, tweet.favorite_count, \n",
        "\t              tweet.in_reply_to_screen_name, tweet.retweeted] for tweet in alltweets]\n",
        "\t\n",
        "\t# write the csv\t\n",
        "\twith open('/content/drive/MyDrive/Project/data/raw/' + '%s_tweets.csv' % screen_name, 'w') as f:\n",
        "\t\twriter = csv.writer(f)\n",
        "\t\twriter.writerow([\"id\",\"created_at\",\"text\",\"likes\",\"in reply to\",\"retweeted\"])\n",
        "\t\twriter.writerows(outtweets)\n",
        "\t\n",
        "\tpass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSZUnmekZkzr"
      },
      "source": [
        "## Data Collection Main Script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVv-CrxiWXRx"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  users = [\"halfmaleficent\", \"bardia_heydari\", \"azdivodadmalool\", \"behdadesfahbod\",\n",
        "           \"elZenakarGrand3\", \"setarebyt\", \"thegarbled\", \"parsa_sann\", \"SaabiTheSab\",\n",
        "           \"afraquotidian\", \"Movaghghati\", \"Parxya\",\n",
        "           \"Checodara\", \"Th3yCallMeMamad\", \"CaveTheNick\",\n",
        "           \"tantanani\", \"60zqueen\",\n",
        "           \"NegarJamalifard\", \"aCarnivalofRust\", \"Sepehr_San\",\n",
        "           \"erfanafre\", \"theparsius\", \"chameleon5421\", \"SheTweeting\"]\n",
        "  for user in users:\n",
        "    try:\n",
        "      get_all_tweets(user)\n",
        "      print(user + \" DONE\")\n",
        "    except tweepy.TweepError:\n",
        "        print(\"Failed to run the command on \" + user)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ3ADQ3mALSF"
      },
      "source": [
        "# Cleaning Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To9713fWAP34"
      },
      "source": [
        "path = '/content/drive/MyDrive/Project/data/raw/'\n",
        "clean_path = '/content/drive/MyDrive/Project/data/clean/'\n",
        "\n",
        "\n",
        "filelist = [f for f in os.listdir(clean_path)]\n",
        "for f in filelist:\n",
        "    os.remove(os.path.join(clean_path, f))\n",
        "\n",
        "normalizer = Normalizer()\n",
        "\n",
        "for filename in os.listdir(path):\n",
        "  if filename.endswith(\".csv\"): \n",
        "    drop_index = []\n",
        "    tweets = pd.read_csv(path + filename)\n",
        "    tweet_body = tweets['text']\n",
        "    for row_index, row in tweets.iterrows():\n",
        "      row = row.copy()\n",
        "      cleaned = row['text'].split()\n",
        "      \n",
        "      created = row['created_at'].split()\n",
        "      created = int(created[0].replace(\"-\", \"\"))\n",
        "      if created < 20201220:\n",
        "        drop_index.append(row_index)\n",
        "        continue\n",
        "      \n",
        "      # break\n",
        "      for item in cleaned:\n",
        "        if '@' in item or 'http' in item or ':' in item or item[0] == '=':\n",
        "          cleaned.remove(item)\n",
        "      glue = ' '\n",
        "      cleaned = glue.join(cleaned)\n",
        "\n",
        "      try:\n",
        "        lang = detect(cleaned)\n",
        "      except:\n",
        "        drop_index.append(row_index)\n",
        "        continue\n",
        "      \n",
        "      if lang != 'fa':\n",
        "        drop_index.append(row_index)\n",
        "        continue\n",
        "\n",
        "      cleaned = normalizer.normalize(cleaned)\n",
        "      tweets.loc[row_index, 'text'] = cleaned\n",
        "\n",
        "    # break\n",
        "    tweets = tweets.drop(drop_index)\n",
        "    tweets.to_csv(clean_path + filename[:-4]+ '_clean.csv')\n",
        "    print(filename + \" CLEANED\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD5ThMq6Su8V"
      },
      "source": [
        "Get first and last tweet's dates to make sure we've chosen the right users."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUhwwRUc7YuS"
      },
      "source": [
        "os.chdir(clean_path)\n",
        "extension = 'csv'\n",
        "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
        "\n",
        "def twtcount(filename):\n",
        "  file = open(filename)\n",
        "  reader = csv.reader(file)\n",
        "  lines= len(list(reader))\n",
        "  return lines\n",
        "\n",
        "sum = 0\n",
        "all = []\n",
        "for f in all_filenames:\n",
        "  test = pd.read_csv(f)\n",
        "  first = test.iloc[-1]\n",
        "  last = test.iloc[0]\n",
        "  all.append((f, first['created_at'], last['created_at']))\n",
        "  # row_count = sum(1 for row in test)\n",
        "  sum += twtcount(f)\n",
        "\n",
        "# all = sort(all)\n",
        "all = sorted(all)\n",
        "for i in all:\n",
        "  print(i)\n",
        "\n",
        "print(sum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkybhLjPtCsj"
      },
      "source": [
        "Combine all raw and clean tweets separately, and sort by date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX1H1J22s8Br"
      },
      "source": [
        "def sort(path_, filename):  \n",
        "  os.chdir(path_)\n",
        "  extension = 'csv'\n",
        "  all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
        "\n",
        "  combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames])\n",
        "  sorted_df = combined_csv.sort_values(by=['created_at'], ascending=True)\n",
        "  sorted_df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
        "  # print(sorted_df.head(3))\n",
        "\n",
        "  file = open(filename)\n",
        "  reader = csv.reader(file)\n",
        "  lines= len(list(reader))\n",
        "  print(lines)\n",
        "\n",
        "sort(path, \"000ALLRAW.csv\")  \n",
        "sort(clean_path, \"000ALLCLEANED.csv\")  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F3PN8igaSrN"
      },
      "source": [
        "# Classification and Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCZf6ehBB1zT"
      },
      "source": [
        "tweets = pd.read_csv(\"/content/drive/MyDrive/Project/data/clean/000ALLCLEANED.csv\")\n",
        "tweets['class'] = \"cold\"\n",
        "tweets['sentences'] = \"\"\n",
        "tweets['words'] = \"\"\n",
        "tweets.to_csv(\"/content/drive/MyDrive/Project/data/clean/000ALLCLEANED_COMPLETE.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ4ZS751aWdI"
      },
      "source": [
        "from hazm import *\n",
        "\n",
        "sentences_list = []\n",
        "words_list = []\n",
        "\n",
        "fulldata_tweets = pd.read_csv(\"/content/drive/MyDrive/Project/data/clean/000ALLCLEANED_COMPLETE.csv\")\n",
        "# tweet_body = fulldata_tweets['text']\n",
        "for row_index, row in fulldata_tweets.iterrows():\n",
        "  row = row.copy()\n",
        "\n",
        "  \n",
        "  created = row['created_at'].split()\n",
        "  created = int(created[0].replace(\"-\", \"\"))\n",
        "\n",
        "  text = row['text']\n",
        "  sent = sent_tokenize(text)\n",
        "  w = word_tokenize(text)\n",
        "  sentences_list.append(sent)\n",
        "  words_list.append(w)\n",
        "\n",
        "\n",
        "  if created > 20210320:\n",
        "    fulldata_tweets.loc[row_index, 'class'] = \"warm\"\n",
        "\n",
        "    # print(row_index)\n",
        "    # print(row[row_index])\n",
        "    # print(row[row_index + 1])\n",
        "\n",
        "    # break\n",
        "fulldata_tweets['sentences'] = sentences_list\n",
        "fulldata_tweets['words'] = words_list\n",
        "\n",
        "fulldata_tweets.to_csv(\"/content/drive/MyDrive/Project/data/clean/000ALLCLEANED_COMPLETE.csv\")\n",
        "print(\"DONE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JxIapQ5ATmI"
      },
      "source": [
        "# Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Qz80AhMQQt7"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EARWQu9ciRI6"
      },
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fulldata = pd.read_csv(\"/content/drive/MyDrive/Project/data/clean/000ALLCLEANED_COMPLETE.csv\")\n",
        "classes = list(fulldata['class'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apOCPB4SQS5F"
      },
      "source": [
        "**Sentence Statistics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-EnHyZUiX-f"
      },
      "source": [
        "sentences = []\n",
        "sent_warm = []\n",
        "sent_cold = []\n",
        "for i in range(len(list(fulldata['sentences']))):\n",
        "    all = list(fulldata['sentences'])[i].split(\"', '\")\n",
        "    for j in all:\n",
        "        if classes[i]=='cold':\n",
        "            sent_cold.append(j)\n",
        "        elif classes[i]=='warm':\n",
        "            sent_warm.append(j)\n",
        "\n",
        "print(\"Number of sentences in class cold: \",len(sent_cold))\n",
        "print(\"Number of sentences in class warm: \",len(sent_warm))\n",
        "print(\"Number of sentences in total: \",len(sent_cold + sent_warm))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeR1e5gNQYWR"
      },
      "source": [
        "**Single-Class Word Statistics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euBRFhq-ktD3"
      },
      "source": [
        "words = []\n",
        "words_cold = []\n",
        "words_warm = []\n",
        "\n",
        "for i in range(len(list(fulldata['words']))):\n",
        "    t = list(fulldata['words'])[i].split(\"', '\")\n",
        "    for j in t:\n",
        "        # print(j)\n",
        "        j = j.replace(\"['\", \"\")\n",
        "        if not j.isascii():\n",
        "          if '@' not in j:\n",
        "            words.append(j)\n",
        "            if classes[i]=='cold':\n",
        "                words_cold.append(j)\n",
        "            elif classes[i]=='warm':\n",
        "                words_warm.append(j)\n",
        "    # break\n",
        "    \n",
        "\n",
        "print(\"Number of words in class cold: \", len(words_cold))\n",
        "print(\"Number of words in class warm: \", len(words_warm))\n",
        "print(\"Number of words in total: \", len(words))\n",
        "\n",
        "print(\"Number of unique words in class cold: \", len(set(words_cold)))\n",
        "print(\"Number of unique words in class warm: \", len(set(words_warm)))\n",
        "print(\"Number of unique words in total: \", len(set(words)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDtTzpc1Qhkz"
      },
      "source": [
        "**Inter-Class Word Statistics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTGibsW1sGeU"
      },
      "source": [
        "cold_unique = set(words_cold)\n",
        "warm_unique = set(words_warm)\n",
        "common_unique = cold_unique & warm_unique\n",
        "unc_unique = cold_unique ^ warm_unique\n",
        "\n",
        "unc_cold = [x for x in words_cold if x not in common_unique]\n",
        "unc_ranking_cold = Counter(unc_cold)\n",
        "unc_warm = [x for x in words_warm if x not in common_unique]\n",
        "unc_ranking_warm = Counter(unc_warm)\n",
        "\n",
        "c_cold = [x for x in words_cold if x in common_unique]\n",
        "c_ranking_cold = Counter(c_cold)\n",
        "c_warm = [x for x in words_warm if x in common_unique]\n",
        "c_ranking_warm = Counter(c_warm)\n",
        "\n",
        "\n",
        "print(\"Number of unique words common between the two classes: \", len(common_unique))\n",
        "print(\"Number of unique words uncommon between the two classes: \", len(unc_unique))\n",
        "print(\"Top 10 uncommon cold words: \", unc_ranking_cold.most_common(10))\n",
        "print(\"Top 10 uncommon warm words: \", unc_ranking_warm.most_common(10))\n",
        "print(\"Top 10 common cold words: \", c_ranking_cold.most_common(10))\n",
        "print(\"Top 10 common warm words: \", c_ranking_warm.most_common(10))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m7MjYQtQBGa"
      },
      "source": [
        "**Relative Normalized Frequency**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iNt0kXZP1Bb"
      },
      "source": [
        "cold_RNF = {}\n",
        "warm_RNF = {}\n",
        "\n",
        "for word in common_unique:\n",
        "    cold_RNF[word]=(c_ranking_cold[word]/len(words_cold))/(c_ranking_warm[word]/len(words_warm))\n",
        "    warm_RNF[word] = (c_ranking_warm[word] / len(words_warm)) / (c_ranking_cold[word] / len(words_cold))\n",
        "\n",
        "cold_RNF=cold_RNF.items()\n",
        "warm_RNF=warm_RNF.items()\n",
        "\n",
        "sorted_cold_RNF = sorted(cold_RNF, key=lambda x: x[1],reverse=True)\n",
        "sorted_warm_RNF = sorted(warm_RNF, key=lambda x: x[1],reverse=True)\n",
        "\n",
        "print(\"Top 10 cold words according to RNF: \", sorted_cold_RNF[:10])\n",
        "print(\"Top 10 warm words according to RNF: \", sorted_warm_RNF[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY0YxdUqQH_k"
      },
      "source": [
        "**TF-IDF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiLMOw6mLcTo"
      },
      "source": [
        "joined_cold = ' '.join(words_cold)\n",
        "joined_warm = ' '.join(words_warm)\n",
        "vectorizer = TfidfVectorizer(stop_words=[\".\", \"که\", \"در\", \"با\", \"به\",\"از\", \"هم\", \"و\", \"رو\", \"یه\", \"،\", \"؟\", \"تا\", \"اون\", \"ولی\", \"برای\", \"«\", \"می\", \"نمی\", \"u200cی\", \"u200cها\", \"u200cهای\"])\n",
        "\n",
        "vectors = vectorizer.fit_transform([joined_cold, joined_warm])\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "dense = vectors.todense()\n",
        "denselist = dense.tolist()\n",
        "\n",
        "df_tfidf = pd.DataFrame(denselist, columns=feature_names)\n",
        "cold_tfidf = df_tfidf.iloc[0]\n",
        "warm_tfidf = df_tfidf.iloc[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owvPwM6pLeKL"
      },
      "source": [
        "print(\"Top 10 cold words according to TDIDF\")\n",
        "\n",
        "cold_tfidf = pd.DataFrame({\"word\": cold_tfidf.index, \"TFIDF\": cold_tfidf.values})\n",
        "cold_tfidf[[\"word\", \"TFIDF\"]].sort_values(\"TFIDF\", ascending = False).head(10)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRyMymlhLglz"
      },
      "source": [
        "print(\"Top 10 warm words according to RNF\")\n",
        "\n",
        "warm_tfidf = pd.DataFrame({\"word\": warm_tfidf.index, \"TFIDF\": warm_tfidf.values})\n",
        "warm_tfidf[[\"word\", \"TFIDF\"]].sort_values(\"TFIDF\", ascending = False).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5VCmr-rWw5O"
      },
      "source": [
        "**Histogram**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gubTIhpLYlYZ"
      },
      "source": [
        "from pandas.plotting import table\n",
        "\n",
        "word_counter = Counter(words)\n",
        "top_list = word_counter.most_common(100)\n",
        "\n",
        "x_labels = [val[0] for val in top_list]\n",
        "y_labels = [val[1] for val in top_list]\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = pd.Series(y_labels).plot(kind='bar')\n",
        "ax.set_xticklabels(x_labels)\n",
        "\n",
        "rects = ax.patches\n",
        "plt.show()\n",
        "\n",
        "df = pd.DataFrame(top_list, columns=[\"Word\", \"Count\"])\n",
        "hist = plt.subplot(111, frame_on=False)\n",
        "hist.xaxis.set_visible(False)\n",
        "hist.yaxis.set_visible(False)\n",
        "\n",
        "table(hist, df, loc='center') \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}